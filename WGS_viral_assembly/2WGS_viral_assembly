#!/bin/bash
###############################################################
## whole Genome Sequence Assembly pipelines for viral genome ##
###############################################################

#NOTE: This assignment is built on the already published raw data from the MPOX outbreak in Kamituga, South Kivu
# You may be able to access the publication by following the link: https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2024.29.11.2400106 

#Step 0: Let us get the raw data to use 

#First we will create the necessary working directory for rw data.

cd $WGS_HOME
echo $WGS_DATA_DIR
mkdir -p $WGS_DATA_DIR
mkdir -p $WGS_REFS_DIR

#Let us change the directory (cd) and go to the created data directory for raw data
cd $WGS_DATA_DIR


#Now check which directory you are in. The terminal should show that you are in the data directory 
pwd

#Step 1: Download raw data and the reference genome in their respective directories
#########################################################################################

#Let us change the directory (cd) and go to the created data directory for raw data
cd $WGS_DATA_DIR

#Make sure you are in the right directory and download the raw data (FASTQ files)

cd $WGS_HOME/data/

#Then we can start downloading the data... NOTE: you can download only one of the 3 provided options and carry on till the end
#The raw data is hosted at the EMBL institute, and you can download it to your local linux system using 'wget' function 

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR126/001/ERR12670101/ERR12670101.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR126/004/ERR12670104/ERR12670104.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR126/005/ERR12670105/ERR12670105.fastq.gz

# Unzip the file to be able to inspect it 


ls #check how many files you have in your folder 

#QUESTION 1A: inspect the file and describe which type of file it is and its relatively important characteristics. 
#            How many reads do we have in that file?
#            You can use the command bellow to count how many reads are there: 

grep -o '@ERR12670' ERR12670104.fastq | wc -l    #How many reads are they? 

#Rename your FASTQ file to easily follow the next steps...
cp ERR12670104.fastq all_reads.fastq

#Let us change the directory (cd) and go to the created directory for reference sequence

cd $WGS_REFS_DIR

#use the 'wget' function to download the reference format 

wget https://www.ebi.ac.uk/ena/browser/api/fasta/JX878425.1?download=true > mpox_ref.fa

#Rename the downloaded reference genome as follows: 
cp JX878425.1?download=true mpox_ref.fa

mkdir -p $WGS_REF_FASTA


#QUESTION 1B: What is the size of the reference genome in terms of number of nucleotides?
#             Which file type is that and how is it different from the FASTQ file? 
#             Does this file contain annotation data? what kind of file would you need if you need the annotation information? 

#Step 2: Quality Control (QC) check using NanoPlot
##################################################

#Create the directory to store quality data
cd $WGS_HOME
echo $WGS_DATA_TRIM_DIR
mkdir -p $WGS_DATA_TRIM_DIR

#Run the command below ensuring that the right <FILE NAME> is provided 

#If the zipped data is not available any more, you can zip it again to be able to run the following command

gzip all_reads.fastq

#Then you can compute the QC using the NanoPlot tool 

NanoPlot --fastq all_reads.fastq.gz -o QC_REPORT --plot kde

#Go to the $WGS_DATA_TRIM_DIR directory and open the generated files to understand the quality status 

open QC_REPORT

#QUESTION 2: Go to the output directory and describe the following: 
#            Mean read length: ____________
#            Mean read quality: ___________
#            Median read length: __________
#            Median read quality:__________
#            Number of reads: _____________
#            Read length N50: _____________
#            Total bases: _________________

#Step 3: Data filtering based on quality score Q() using fastp
##############################################################

cd $WGS_DATA_TRIM_DIR

#If your fastq file is not unzipped, please use the commanda below to unzip it 

gunzip all_reads.fastq.gz

# Then filter the fastq file to remove short and low quality reads 

fastp -w 48 -i all_reads.fastq -l 100 -q 9 -o all_readsQC.fastq

#QUESTION 3: Please report the values below, from the data you find: 
#total reads: ____________
#total bases: _____________
#Q20 bases: ____________%____
#Q30 bases: ____________%______

#Read1 after filtering:
#total reads:_________
#total bases: __________
#Q20 bases: _________%_________
#Q30 bases: __________%________

#Filtering result:_____________
#reads passed filter: _________________
#reads failed due to low quality:________
#reads failed due to too many N: __________
#reads failed due to too short:_______________
#reads with adapter trimmed: ____________
#bases trimmed due to adapters:_______________

#Duplication rate (may be overestimated since this is SE data): ____________

#Step 4: Trim adapters from the first and the last end
######################################################

#Check if the cutadapt tool is installed and if not install using the command bellow: 

sudo apt install cutadapt

#Trim adapters from the first end of the reads

cutadapt -u 30 -o all_reads_QC1.fastq $WGS_DATA_TRIM_DIR/all_reads_QC.fastq

# Trim adapters from the other end of the reads

cutadapt -u -30 -o all_reads_QC2.fastq $WGS_DATA_TRIM_DIR/all_reads_QC1.fastq

QUESTION 4: Compare Total basepairs processed in both 'cutadapt' steps. Are they the same or different? Why? 


#Step 5: Align reads to the reference genome using minimap2 and generate SAM file
#################################################################################

#We need to make sure that we are in the right directories 
echo $WGS_HOME
We will create the directory to save alignment data
mkdir -p $WGS_ALIGN_DIR
cd $WGS_ALIGN_DIR

# Align reads to the reference genome by providing the right PATH to your data and the reference and generate SAM file
minimap2 -Y -t 12 -x map-ont -a $WGS_REF_FASTA $WGS_DATA_TRIM_DIR/all_reads_QC2.fastq > all_reads.sam

#QUESTION 5: Inspect and describe the obtained SAM file, and answer to the following questions: 
#            What are the characteristic features of SAM file? 
#            using the head function try to describe the obtained sam file using the key features
#            


#Step 6: Sort the SAM file
############################

# Sort the SAM file
samtools sort -O SAM -o all_reads_sorted.sam all_reads.sam

#QUESTION: using the generated sorted sam file as an example, why do we need to sort the SAM file? 

#Step 7: Convert SAM to BAM and sort
####################################

# Convert SAM to BAM and sort the BAM file
samtools view -bS all_reads_sorted.sam | samtools sort -@ 16 -o all_reads.bam


#QUESTION 7: What is the main difference between SAM and BAM files? Why is the conversion of SAM to BAM important? 

#Step 8: Index the BAM file
############################

# Index the BAM file
samtools index all_reads.bam

#Step 9: Create a depth profile
################################

# Create a depth profile of the aligned reads
samtools mpileup -a -A -Q 0 -d 0 -f $WGS_REF_FASTA all_reads.bam | awk '{print "$2","$3","$4"}' > all_reads.depth

#Step 10: Variant calling
#########################

# Call variants using bcftools
bcftools mpileup -f $WGS_REF_FASTA all_reads.bam | bcftools call -mv -Oz -o all_reads.vcf.gz

#Step 11: Filter the VCF file
##############################

# Filter the VCF file using bcftools
bcftools filter -i 'DP>20 && AF>0.1' all_reads.vcf.gz -Oz -o all_reads_filtered.vcf.gz

#Step 12: Create a consensus sequence
######################################


# Index the reference genome
samtools faidx /Users/JeanPierre/Documents/Bioinformatics/NRL_SARS_CoV2/barcode25/SARS_Cov2_refs/GCA_009858895.3.fasta

# Generate consensus sequence with bcftools consensus
bcftools consensus -f /Users/JeanPierre/Documents/Bioinformatics/barcode25/SARS_Cov2_refs/GCA_009858895.3.fasta all_reads_filtered.vcf.gz > all_reads_consensus_temp.fasta

# Incorporate depth profile into the consensus sequence
awk -F, -v depth_file=all_reads.depth 'BEGIN {
  while ((getline < depth_file) > 0) {
    depth[$1] = $3
  }
}
{
  if (/^>/) {
    print $0
  } else {
    seq = $0
    for (i = 1; i <= length(seq); i++) {
      if (depth[i] < 20) {
        seq = substr(seq, 1, i-1) "N" substr(seq, i+1)
      }
    }
    print seq
  }
}' all_reads_consensus_temp.fasta > all_reads_consensus.fasta
